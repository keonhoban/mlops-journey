<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Airflow | 🏔️ MLOps Journey</title>
<meta name=keywords content><meta name=description content><meta name=author content><link rel=canonical href=https://keonhoban.github.io/mlops-journey/categories/airflow/><link crossorigin=anonymous href=/mlops-journey/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://keonhoban.github.io/mlops-journey/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://keonhoban.github.io/mlops-journey/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://keonhoban.github.io/mlops-journey/favicon-32x32.png><link rel=apple-touch-icon href=https://keonhoban.github.io/mlops-journey/apple-touch-icon.png><link rel=mask-icon href=https://keonhoban.github.io/mlops-journey/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://keonhoban.github.io/mlops-journey/categories/airflow/index.xml><link rel=alternate hreflang=en href=https://keonhoban.github.io/mlops-journey/categories/airflow/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://keonhoban.github.io/mlops-journey/categories/airflow/"><meta property="og:site_name" content="🏔️  MLOps Journey"><meta property="og:title" content="Airflow"><meta property="og:locale" content="ko"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Airflow"><meta name=twitter:description content></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://keonhoban.github.io/mlops-journey/ accesskey=h title="🏔️  MLOps Journey (Alt + H)">🏔️ MLOps Journey</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://keonhoban.github.io/mlops-journey/ title="🏠 Home"><span>🏠 Home</span></a></li><li><a href=https://keonhoban.github.io/mlops-journey/projects/ title="📂 Projects"><span>📂 Projects</span></a></li><li><a href=https://keonhoban.github.io/mlops-journey/posts/ title="📝 Blog"><span>📝 Blog</span></a></li><li><a href=https://keonhoban.github.io/mlops-journey/about/ title="🧗 About"><span>🧗 About</span></a></li><li><a href=https://keonhoban.github.io/mlops-journey/categories/ title="📖 Categories"><span>📖 Categories</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://keonhoban.github.io/mlops-journey/>Home</a>&nbsp;»&nbsp;<a href=https://keonhoban.github.io/mlops-journey/categories/>Categories</a></div><h1>Airflow</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[MLOps 플랫폼 구축 : Airflow-MLflow-FastAPI (Helm)]</h2></header><div class=entry-content><p>📌 전체 시리즈 요약 순서 주제 1 🔗 인프라 사전 구성 (Kubernetes, NFS, PostgreSQL, S3 등) 2 🔗 Secret/보안 구성 및 Kubernetes 연동 3 🔗 MLflow Tracking 서버 및 Registry 구축 4 🔗 Airflow DAG Git 연동 및 Secret 기반 구성 5 🔗 FastAPI 모델 서빙 & MLflow 모델 자동 로딩 6 🔗 Airflow + MLflow + FastAPI 연결을 통해 모델 핫스왑 💡 지금까지 구현한 아키텍처 요약 🎯 실무 관점에서 강점 항목 내용 모델 실험 자동화 Airflow DAG + MLflow 연동으로 다양한 모델 버전 학습 자동화 서빙 안정성 FastAPI가 Production Stage 기준으로 모델 로드 → 무중단 핫스왑 가능 보안 구성 AWS 인증 정보 및 DB 정보는 Secret으로 주입 인프라 이식성 Helm + Docker + Kubernetes 기반 → 어디서든 이식 가능 실시간 추론 확인 Ingress 기반 UI/Endpoint 연결 → 바로 curl 테스트 가능 🔍 회고 ✅ 프로덕션 환경 고려 운영 가능한 MLOps 구조로 설계 (유지보수/보안 고려) AWS S3, PostgreSQL, Kubernetes, GitOps까지 현실 환경 가정하고 구성 Secret 설계, Volume 마운트, GitSync, 커스텀 이미지 등 세세한 부분까지 설계 주도 ✅ 자동화 기반 설계 Airflow를 통해 모델 학습 → Registry 등록 → 서빙까지 자동화 모델 핫스왑 실험까지 성공적으로 구현 🧱 부족했던 점 & 보완 계획 항목 개선 포인트 모니터링 Prometheus + Grafana로 서빙/실험 성능 모니터링 추가 필요 모델 테스트 자동화 pytest + CI 파이프라인 구성으로 품질 확보 고려 Kubeflow 연계 Kubeflow Pipelines 및 Katib 등과의 비교 분석 예정 Terraform 기반 전환 Helm 구성 요소를 코드로 관리하는 Terraform 인프라 전환 계획 ✨ 마치며 이번 시리즈는 단순한 학습이 아니라, 설계에 대해 고민하며
...</p></div><footer class=entry-footer><span title='2025-07-15 17:55:05 +0900 +0900'>July 15, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [MLOps 플랫폼 구축 : Airflow-MLflow-FastAPI (Helm)]" href=https://keonhoban.github.io/mlops-journey/projects/mlops_pipeline/helm/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[MLOps 플랫폼 구축 - 6단계: 실시간 모델 핫스왑 구조 실험]</h2></header><div class=entry-content><p>✅ TL;DR Airflow에서 선택한 모델 버전에 따라 학습 스크립트를 다르게 실행 학습된 모델을 MLflow Model Registry에 등록 → Production 단계로 자동 승격 FastAPI 서버는 Production 모델을 다시 로드 → 별도 코드 수정 없이 핫스왑 완료 실무에서 모델 검증/배포 사이클 자동화에 바로 응용 가능 🧠 구조 다이어그램 (핫스왑 흐름) 🧩 핵심 구성 요소 1. Airflow DAG - 모델 버전 선택 # dags/train_promote_pipeline_share.py from airflow import DAG from airflow.operators.bash import BashOperator from airflow.models import Variable from datetime import datetime default_args = { 'start_date': datetime(2023, 1, 1), } with DAG( dag_id='train_promote_pipeline_share', default_args=default_args, schedule=None, catchup=False, tags=["ml", "train"] ) as dag: # moedel_version 변수 default_var 지정 model_version = Variable.get("model_version", default_var="v1") debug_aws = BashOperator( task_id='debug_aws_credentials', bash_command='echo $HOME && ls -al $HOME/.aws && cat $HOME/.aws/credentials', env={"HOME": "/home/airflow"}, ) run_train_script = BashOperator( task_id='run_train_script', # merdel_version 변수화 bash_command=f'python /opt/airflow/dags/repo/ml_code/train_model_{model_version}.py', env={"HOME": "/home/airflow"}, ) promote_model = BashOperator( task_id='promote_model_to_production', bash_command='python /opt/airflow/dags/repo/ml_code/promote_model.py', env={"HOME": "/home/airflow"}, ) debug_aws >> run_train_script >> promote_model → Airflow UI > Admin > Variables 에서 model_version = v1 또는 v2 설정 가능
...</p></div><footer class=entry-footer><span title='2025-07-10 17:12:00 +0900 +0900'>July 10, 2025</span>&nbsp;·&nbsp;3 min</footer><a class=entry-link aria-label="post link to [MLOps 플랫폼 구축 - 6단계: 실시간 모델 핫스왑 구조 실험]" href=https://keonhoban.github.io/mlops-journey/posts/mlops-pipeline-helm/06/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[MLOps 플랫폼 구축 - 4단계: Airflow : GitSync + 외부 PostgreSQL + Secret 연동]</h2></header><div class=entry-content><p>✨ TL;DR Helm을 통해 Airflow를 배포하면서 DAG 코드를 Git 저장소에서 자동으로 동기화하는 구조 설계 GitSync, Secret 기반 SSH 인증, 외부 PostgreSQL, AWS S3 연동까지 포함해 구성 UI 접근은 Ingress를 통해 이루어지며, 로그는 PVC 또는 S3로 설정 가능 📐 아키텍처 구성도 🐳 커스텀 Airflow 이미지 구성 GitSync DAG에서 MLflow 연동 또는 AWS SDK 사용을 위한 Python 패키지 설치 필요
🔸 Dockerfile FROM apache/airflow:3.0.2-python3.12 COPY requirements.txt /tmp/requirements.txt USER airflow RUN pip install --no-cache-dir -r /tmp/requirements.txt 🔸 requirements.txt mlflow==2.13.0 pandas scikit-learn boto3 🔸 빌드 & 푸시 docker build -t hoizz/airflow-custom:v5-mlflow . docker push hoizz/airflow-custom:v5-mlflow 🛠 Helm 구성 🔹 values.yaml # 실행 엔진 설정 executor: KubernetesExecutor # 이미지 설정 images: airflow: repository: hoizz/airflow-custom # 커스텀 이미지 tag: v5-mlflow-20240712 # 태그 pullPolicy: IfNotPresent # 생성한 fernetKey 고정 (helm delete & install 이후에도 이전 variables 등 데이터 sync 가능) fernetKeySecretName: my-fernet-secret # DAG GitSync 설정 (GitOps 기반) dags: gitSync: enabled: true repo: git@github.com:keonhoban/airflow-dags.git branch: main subPath: dags depth: 1 wait: 10 rev: HEAD sshKeySecret: airflow-git-ssh-secret # DB 연동 (외부 PostgreSQL 사용) postgresql: enabled: false # 내부 Postgres 비활성화 data: metadataSecretName: airflow-db-secret # Secret 안의 base64된 connection 값 참조 # Airflow 환경 설정 config: AIRFLOW__CORE__LOAD_EXAMPLES: value: "False" AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: value: "False" AIRFLOW__WEBSERVER__EXPOSE_CONFIG: value: "True" # 리소스 설정 (권장) scheduler: resources: requests: cpu: "500m" memory: "512Mi" limits: cpu: "1" memory: "1Gi" extraVolumes: - name: aws-credentials secret: secretName: aws-credentials-secret extraVolumeMounts: - name: aws-credentials mountPath: /home/airflow/.aws readOnly: true workers: resources: requests: cpu: "500m" memory: "512Mi" limits: cpu: "1" memory: "1Gi" extraVolumes: - name: aws-credentials secret: secretName: aws-credentials-secret extraVolumeMounts: - name: aws-credentials mountPath: /home/airflow/.aws readOnly: true dagProcessor: extraVolumes: - name: aws-credentials secret: secretName: aws-credentials-secret extraVolumeMounts: - name: aws-credentials mountPath: /home/airflow/.aws readOnly: true # Ingress 설정 (Ingress Controller 기반 접근) ingress: enabled: true apiServer: enabled: true ingressClassName: nginx hosts: - name: airflow.local tls: enabled: false # 실무에서는 true + cert-manager 연동 필요 # Web UI 서비스 타입 설정 apiServer: service: type: ClusterIP # 외부 접근은 Ingress 경유 # 로그 PVC 설정 logs: persistence: enabled: true existingClaim: airflow-logs-pvc # (선택) remote 로그 저장소 설정 (예: S3) # config: # AIRFLOW__LOGGING__REMOTE_LOGGING: "True" # AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://your-bucket-name/airflow-logs" # AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "aws_default" ✅ fernet_key 생성 및 구성 # 생성 python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())" # secret 적용 (파일 생성 후 kubectl apply 가능) # (git에 필요한 경우 SealedSecret or SOPS or .gitignore) apiVersion: v1 kind: Secret metadata: name: my-fernet-secret namespace: airflow type: Opaque stringData: fernet-key: "xZ69xLVkiCfJQ23fXa8oubANRIJRHZLrIeANlHhPGmQ=" # 생성한 값으로 수정 🔐 GitSync SSH Key & Secret 구성 # SSH 키 생성 ssh-keygen -t rsa -b 4096 -C "testuser@mlops.com" # GitHub UI에서 Deploy Key 등록 (공개키) # K8s Secret 생성 (비공개 키) kubectl create secret generic airflow-git-ssh-secret --from-file=gitSshKey=/root/.ssh/id_rsa -n airflow 🚀 배포 명령어 kubectl create namespace airflow helm install airflow apache-airflow/airflow -n airflow -f values.yaml 🌐 접속 확인 # /etc/hosts 수정 {NodeIP} airflow.local # 브라우저 접속 http://airflow.local 🧪 확인 사항 항목 확인 방법 Airflow UI 접근 http://airflow.local DAG GitSync GitHub push → 자동 반영 여부 확인 PostgreSQL 연결 metadata DB에 연결 로그 확인 AWS 인증 연동 boto3.client() 호출 시 에러 없는지 확인 🧩 Tip GitSync는 depth: 1, wait, rev: HEAD 옵션 설정으로 최적화 필요 metadataSecretName에는 AIRFLOW__DATABASE__SQL_ALCHEMY_CONN이 base64 인코딩되어 있어야 함 각 Pod (scheduler, worker, dagProcessor)마다 AWS credential 마운트 필요 Ingress 접근을 위해선 hosts 파일에 수동 등록 필요 (로컬 클러스터인 경우) 🔧 MLOps 실전 연결 연결 항목 실무 의미 DAG GitSync GitOps 기반으로 실험/운영 동기화 자동화 External PostgreSQL 운영 DB와 통합, 백업/모니터링 가능 boto3 연동 DAG 내 S3 접근 가능 (모델 or raw 데이터 연동) mlflow API 사용 실험 자동화 가능 (training, promotion 포함) 🚨 트러블슈팅 🔻 GitSync DAG 미반영시 체크 리스트 subPath 경로 오류 Secret에 등록된 SSH key 권한 문제 GitHub deploy key에 write 권한 누락 🧭 다음 포스트 예고 ⚙️ FastAPI 서빙 + MLflow 모델 연동
...</p></div><footer class=entry-footer><span title='2025-07-03 17:11:53 +0900 +0900'>July 3, 2025</span>&nbsp;·&nbsp;3 min</footer><a class=entry-link aria-label="post link to [MLOps 플랫폼 구축 - 4단계: Airflow : GitSync + 외부 PostgreSQL + Secret 연동]" href=https://keonhoban.github.io/mlops-journey/posts/mlops-pipeline-helm/04/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[Airflow 기초 자동화 - Airflow → MLflow → FastAPI]</h2></header><div class=entry-content><p>🧭 전체 흐름 예시 [AIRFLOW DAG 실행] ↓ [train_mlflow.py] - iris 모델 학습 - 파라미터/메트릭 로깅 - 모델 Registry 등록 ↓ [promote_mlflow.py] - 최신 모델을 Production으로 전환 ↓ [FastAPI] - models:/IrisModel/Production → 실시간 예측 👉 실습 코드는 🔗 GitHub (Airflow + MLflow + FastAPI)
✅ [1단계] 프로젝트 기본 폴더 구조 설계 📁 1. 전체 디렉토리 구성도 mlops_project/ ├── airflow/ 🛫 Airflow 설정 및 DAG 스케줄러 │ ├── dags/ ← DAG 정의 디렉토리 │ │ └── train_with_mlflow.py ← 학습 DAG (MLflow 연동) │ ├── Dockerfile.airflow ← Airflow용 Dockerfile │ ├── requirements.txt ← Airflow 의존성 │ └── .dockerignore │ ├── fastapi/ ⚡ FastAPI 예측 API 서버 │ ├── app/ │ │ └── main.py ← 모델 서빙 엔드포인트 │ ├── Dockerfile.api ← FastAPI용 Dockerfile │ ├── requirements.txt ← FastAPI 의존성 │ └── .dockerignore │ ├── ml_code/ 🧠 ML 학습 및 프로모션 코드 │ ├── train_mlflow.py ← 모델 학습 및 MLflow 로깅 │ └── promte_mlflow.py ← 모델 프로모션 (Staging → Production) │ ├── mlflow_store/ 🗂️ MLflow 저장소 경로 (볼륨) │ ├── Dockerfile.mlflow ← MLflow 서버 커스터마이징 │ ├── mlflow.db ← Model Registry DB (sqlite) │ ├── mlruns/ ← 실험 로그 디렉토리 │ ├── artifacts/ ← 모델 아티팩트 저장소 │ └── .dockerignore │ ├── docker-compose.yaml 🧩 전체 서비스 구성 정의 ├── .env 🔐 민감 정보 (.env로 분리) ├── README.md 📝 전체 프로젝트 문서화 ├── .gitignore └── .dockerignore ✅ [2단계] docker-compose.yaml 통합 구성 🧭 구성 목표 서비스명 설명 포트 airflow DAG 실행 환경 (webserver/scheduler) 8080 postgres Airflow 메타데이터 저장용 DB 내부 통신 mlflow MLflow UI + Registry 기능 5000 fastapi 추론 API 서버 (모델 로딩) 8000 이미지 사용시 주의 (UI만 제공하는 이미지 존재) 📄 docker-compose.yaml 전체 예시 version: '3.8' services: # 📦 PostgreSQL: Airflow 메타데이터 저장용 DB postgres: image: postgres:13 container_name: postgres env_file: - .env # ← 민감정보 분리 (아이디/비번) environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_DB: ${POSTGRES_DB} volumes: # ← 코드/데이터 공유 및 영속성 보장 - postgres_data:/var/lib/postgresql/data # ← DB 데이터 유지 (재시작 대비) # 🛫 Airflow: DAG 스케줄러 및 태스크 실행 airflow: build: context: ./airflow # → Airflow 전용 Dockerfile 경로 dockerfile: Dockerfile.airflow container_name: airflow command: standalone # → 로컬 테스트용 간단 실행 명령 # (- Scheduler + Webserver + DB 초기화까지 자동으로 한번에 실행) # (- 실무/운영에서는 airflow-webserver, airflow-scheduler 필드 분리) ports: - "8080:8080" # → Airflow 웹 UI (localhost:8080) depends_on: - postgres # → DB가 먼저 올라와야 Airflow 시작 가능 env_file: - .env environment: # Airflow 메타데이터 DB 연결 주소 AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW__CORE__SQL_ALCHEMY_CONN} # Airflow 예제 DAG 불러올지 여부 AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES} MLFLOW_TRACKING_URI: http://mlflow:5000 # → DAG 코드에서 MLflow 연동 volumes: - ./airflow/dags:/opt/airflow/dags # DAG 파일 mount - ./ml_code:/opt/airflow/ml_code # 학습 코드 공유 - ./mlflow_store:/mlflow # 모델 저장소 공유 # 🔬 MLflow: 실험 추적 + 모델 레지스트리 서버 mlflow: build: context: ./mlflow_store # 커스텀 Dockerfile 위치 dockerfile: Dockerfile.mlflow ports: - "5000:5000" # → MLflow UI (localhost:5000) volumes: - ./mlflow_store:/mlflow # 실험 로그 + DB + artifacts 저장 environment: - MLFLOW_TRACKING_URI=http://0.0.0.0:5000 # 내부 컨테이너 기준 URI # ⚡ FastAPI: 모델 서빙 API fastapi: build: context: ./fastapi dockerfile: Dockerfile.api container_name: fastapi ports: - "8000:8000" # → 예측 API 엔드포인트 (localhost:8000) volumes: - ./fastapi/app:/app/app # FastAPI app 디렉토리 mount - ./ml_code:/app/ml_code # 학습/모델 코드 공유 - ./mlflow_store:/mlflow # 저장된 모델 불러오기 위한 mount # 🗂️ 볼륨 정의 (Postgres DB 영속성 유지) volumes: postgres_data: 🎁 추가로 해야 할 것 Airflow 첫 실행 후엔 보통 관리자 계정 생성도 해줘야 함: # airflow 컨테이너 접속 docker exec -it airflow bash # 관리자 계정 생성 airflow users create \ --username airflow \ --password airflow \ --firstname Keoho \ --lastname Ban \ --role Admin \ --email airflow@example.com 🔁 [구축 Tip] Airflow, FastAPI, MLflow 간 공유 볼륨 구조 확인 공유 리소스 설명 ./mlflow_store:/mlflow (MLflow) MLflow 서버가 쓰는 로그/모델 저장소 ./mlflow_store:/mlflow (Airflow) 학습 후 모델 저장 위치 공유 ./mlflow_store:/mlflow (FastAPI) 모델 추론 시 로드 경로 공유 ➡ 경로 통일성이 매우 중요함! 지금은 모두 ./mlflow로 공유 (./mlflow 하위에 /mlruns 존재)
...</p></div><footer class=entry-footer><span title='2025-06-13 20:58:51 +0900 +0900'>June 13, 2025</span>&nbsp;·&nbsp;8 min</footer><a class=entry-link aria-label="post link to [Airflow 기초 자동화 - Airflow → MLflow → FastAPI]" href=https://keonhoban.github.io/mlops-journey/projects/mlops_pipeline/basic/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[Airflow - 5단계: PythonOperator + MLflow Tracking 연동]</h2></header><div class=entry-content><p>목표
PythonOperator를 사용하여 MLflow로 실험(모델 학습)을 자동화하고, 파라미터, 메트릭, 모델을 기록하는 방법을 학습합니다.
👉 실습 코드는 🔗 GitHub (Mlflow - Tracking + FastAPI)
🧭 실습 전체 흐름 요약 [1단계] MLflow 실험 스크립트 작성 [2단계] Airflow DAG 구성 [3단계] DAG 실행 및 파라미터/메트릭 확인 [4단계] 모델 저장 및 로깅 상태 점검 📁 디렉토리 구조 airflow/ ├── dags/ │ ├── train_with_mlflow.py ← DAG 파일 ├── ml_code/ │ ├── train_mlflow.py ← MLflow 연동 학습 스크립트 └── mlruns/ ← MLflow 로깅 결과 저장 폴더 (자동 생성) 🧪 1단계: MLflow 학습 스크립트 작성 # airflow/ml_code/train_mlflow.py import mlflow import mlflow.sklearn from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def run_experiment(): mlflow.set_tracking_uri("file:/opt/airflow/mlruns") mlflow.set_experiment("airflow_mlflow_example") with mlflow.start_run(): # 데이터 로딩 data = load_iris() X, y = data.data, data.target # 모델 정의 model = RandomForestClassifier(n_estimators=50, max_depth=3) model.fit(X, y) preds = model.predict(X) acc = accuracy_score(y, preds) # MLflow 기록 mlflow.log_param("n_estimators", 50) mlflow.log_param("max_depth", 3) mlflow.log_metric("accuracy", acc) mlflow.sklearn.log_model(model, "model") 🧪 2단계: Airflow DAG 작성 # airflow/dags/train_with_mlflow.py from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime import sys sys.path.append("/opt/airflow/ml_code") from train_mlflow import run_experiment with DAG( dag_id='mlflow_tracking_dag', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, ) as dag: run_mlflow = PythonOperator( task_id='run_mlflow_training', python_callable=run_experiment, ) ✅ 실행 절차 train_mlflow.py 작성 train_with_mlflow.py DAG 등록 Airflow UI에서 DAG 실행 Task 로그에서 파라미터/메트릭/모델 기록 확인 ❓모듈 에러가 발생하면? docker-compose.yaml에서 volumes: 항목 확인
...</p></div><footer class=entry-footer><span title='2025-06-10 19:49:29 +0900 +0900'>June 10, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [Airflow - 5단계: PythonOperator + MLflow Tracking 연동]" href=https://keonhoban.github.io/mlops-journey/posts/airflow/05/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[Airflow - 4단계: BashOperator로 외부 Python 학습 스크립트 실행]</h2></header><div class=entry-content><p>목표
train.py 모델 학습 스크립트를 별도 파일로 작성 Airflow DAG에서 해당 스크립트를 BashOperator로 실행 모델 학습 및 결과 확인 👉 실습 코드는 🔗 GitHub (Mlflow - Tracking + FastAPI)
🧭 실습 전체 흐름 요약 [1단계] 학습 스크립트 작성 [2단계] BashOperator로 스크립트 실행 DAG 구성 [3단계] Airflow 웹 UI에서 실행 및 로그 확인 📁 디렉토리 구조 airflow/ ├── dags/ │ └── run_train_script.py ← DAG 파일 ├── ml_code/ │ ├── train.py ← 모델 학습 스크립트 │ └── model.pkl ← 학습된 모델 파일 🧪 [1단계] 학습 스크립트 작성 (train.py) # airflow/ml_code/train.py import pickle from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier # 데이터 로딩 data = load_iris() X, y = data.data, data.target # 모델 학습 model = RandomForestClassifier() model.fit(X, y) # 모델 저장 model_path = "/opt/airflow/ml_code/model.pkl" with open(model_path, "wb") as f: pickle.dump(model, f) 모델은 /opt/airflow/ml_code/model.pkl 경로에 저장됩니다.
...</p></div><footer class=entry-footer><span title='2025-06-10 19:49:27 +0900 +0900'>June 10, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [Airflow - 4단계: BashOperator로 외부 Python 학습 스크립트 실행]" href=https://keonhoban.github.io/mlops-journey/posts/airflow/04/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[Airflow - 3단계: ML 파이프라인 DAG 구성]</h2></header><div class=entry-content><p>목표
ML 워크플로우를 DAG 형태로 구성하여
데이터 준비 → 모델 학습 → 모델 저장 흐름을 시뮬레이션함
👉 실습 코드는 🔗 GitHub (Mlflow - Tracking + FastAPI)
🧭 실습 전체 흐름 요약 ① load_data (가상 데이터 경로 리턴) ② train_model (데이터 경로 받아 학습 흉내) ③ save_model (모델 경로 받아 저장 완료 메시지) → XCom을 통해 단계별 결과 전달 📁 실습 디렉토리 예시 airflow/ └── dags/ └── ml_simulation.py 🧪 실습 코드 (ml_simulation.py) from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime def load_data(): print("📥 데이터 로딩 완료 (가상)") return {"data_path": "/tmp/fake_data.csv"} def train_model(**context): data = context['ti'].xcom_pull(task_ids='load_data') print(f"🧪 데이터 경로: {data['data_path']}") print("🚀 모델 학습 완료 (가상)") return {"model_path": "/tmp/fake_model.pkl"} def save_model(**context): model = context['ti'].xcom_pull(task_ids='train_model') print(f"💾 모델 저장 경로: {model['model_path']}") print("✅ 저장 완료 (가상)") with DAG( dag_id='ml_simulation', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False ) as dag: t1 = PythonOperator(task_id='load_data', python_callable=load_data) t2 = PythonOperator(task_id='train_model', python_callable=train_model, provide_context=True) t3 = PythonOperator(task_id='save_model', python_callable=save_model, provide_context=True) t1 >> t2 >> t3 ✅ 저장 경로: dags/ml_simulation.py
...</p></div><footer class=entry-footer><span title='2025-06-07 19:30:16 +0900 +0900'>June 7, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [Airflow - 3단계: ML 파이프라인 DAG 구성]" href=https://keonhoban.github.io/mlops-journey/posts/airflow/03/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[Airflow - 2단계: Python & Bash Operator + XCom 데이터 전달]</h2></header><div class=entry-content><p>목표
Python 함수와 Bash 스크립트를 하나의 DAG으로 구성 XCom을 활용한 Task 간 데이터 전달 체험 Web UI에서 실행 흐름과 로그 확인 👉 실습 코드는 🔗 GitHub (Mlflow - Tracking + FastAPI)
🧭 실습 전체 흐름 요약 ① DAG 생성: PythonOperator + BashOperator 조합 ② XCom으로 태스크 간 메시지 전달 ③ 로그로 전달 메시지 확인 ④ 전체 DAG 실행 및 의존성 확인 📁 DAG 파일 구조 airflow/ ├── dags/ │ └── python_bash_xcom.py ← 여기 저장 └── docker-compose.yaml 🧱 DAG 코드 예시 from airflow import DAG from airflow.operators.python import PythonOperator from airflow.operators.bash import BashOperator from datetime import datetime def generate_message(): return "🌟 Hello from PythonOperator!" def print_xcom_message(**context): msg = context['ti'].xcom_pull(task_ids='generate_task') print(f"📬 XCom received message: {msg}") with DAG( dag_id='python_bash_xcom', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, ) as dag: generate_task = PythonOperator( task_id='generate_task', python_callable=generate_message, ) consume_task = PythonOperator( task_id='consume_task', python_callable=print_xcom_message, provide_context=True, ) bash_task = BashOperator( task_id='bash_echo', bash_command="echo '🎉 Bash task is running!'" ) generate_task >> consume_task >> bash_task 🧪 실행 방법 요약 docker-compose up -d # Airflow 실행 중인지 확인 브라우저 접속: http://localhost:8080 DAG 목록 → python_bash_xcom ON ▶ 버튼 클릭 → Trigger DAG 각 Task 클릭 → Log 탭에서 실행 결과 확인 📊 결과 확인 포인트 Task 로그에서 확인 내용 generate_task "🌟 Hello from PythonOperator!" 메시지 리턴 consume_task 📬 XCom received message: 출력 확인 bash_task '🎉 Bash task is running!' 로그 확인 🧩 실무 팁 XCom은 간단한 문자열/경로/ID 등 소형 데이터 전달에 적합 대용량 결과는 S3/DB에 저장 후 경로만 XCom으로 전달하는 방식 추천 🔧 MLOps 실전 연결 실무 시나리오 Airflow 사용 방식 학습 결과 저장 train_model → register_model 태스크로 XCom 전달 태스크 연결 흐름 추적 Graph View에서 DAG 시각화로 관리 후속 작업 자동화 BashOperator로 배포 스크립트 실행 등 자동화 가능</p></div><footer class=entry-footer><span title='2025-06-07 19:30:15 +0900 +0900'>June 7, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [Airflow - 2단계: Python & Bash Operator &#43; XCom 데이터 전달]" href=https://keonhoban.github.io/mlops-journey/posts/airflow/02/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[Airflow - 1단계: 로컬 환경에서 기본 DAG 실행]</h2></header><div class=entry-content><p>목표
Docker 기반 Airflow 환경 구성 DAG 파일을 작성하고 실행 UI에서 워크플로우 흐름과 로그를 직접 확인 👉 실습 코드는 🔗 GitHub (Mlflow - Tracking + FastAPI)
🧭 실습 전체 흐름 요약 ① Docker 설치 확인 ② 공식 Airflow 예제 다운로드 ③ docker-compose 실행 ④ DAG UI 접속 및 실행 ⑤ 로그 확인으로 정상 여부 검증 📁 실습 디렉토리 구조 airflow/ ├── dags/ # DAG 파일 작성 위치 ├── logs/ # 작업 로그 저장 ├── plugins/ # 커스텀 플러그인 (선택) ├── docker-compose.yaml └── .env # AIRFLOW_UID 포함 🔧 주요 명령어 정리 # Airflow 예제 다운로드 git clone https://github.com/apache/airflow.git cd airflow/dev && ./docker-compose/setup.sh # 또는 간단한 버전 curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.2/docker-compose.yaml' mkdir -p ./dags ./logs ./plugins echo -e "AIRFLOW_UID=$(id -u)" > .env # 서비스 실행 docker-compose up -d # 접속 http://localhost:8080 (ID/PW: airflow / airflow) 💡 샘플 DAG 예시 from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime with DAG(dag_id="hello_airflow", start_date=datetime(2023, 1, 1), schedule_interval="@daily", catchup=False) as dag: t1 = BashOperator(task_id="print_date", bash_command="date") t2 = BashOperator(task_id="say_hello", bash_command="echo 'Hello, Airflow!'") t1 >> t2 👉 dags/hello_airflow.py 로 저장
...</p></div><footer class=entry-footer><span title='2025-06-07 19:29:36 +0900 +0900'>June 7, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [Airflow - 1단계: 로컬 환경에서 기본 DAG 실행]" href=https://keonhoban.github.io/mlops-journey/posts/airflow/01/></a></article></main><footer class=footer><p style=margin-top:1rem>© 2025 Keonho Ban | <a href=https://github.com/keonhoban target=_blank>GitHub</a> | <a href=mailto:keonho0510@naver.com>Email</a></p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>