+++
date = '2025-06-10T19:49:27+09:00'
draft = false
title = '[Airflow - 4단계: BashOperator로 외부 Python 학습 스크립트 실행]'
categories = 'Airflow'
+++

> 목표
> 
> 1. `train.py` 모델 학습 스크립트를 별도 파일로 작성
> 2. Airflow DAG에서 해당 스크립트를 `BashOperator`로 실행
> 3. 모델 학습 및 결과 확인

👉 실습 코드는 [🔗 GitHub (Mlflow - Tracking + FastAPI)](https://github.com/keonhoban/mlops-infra-labs/tree/main/airflow/04_BashOperator_and_Python_ML_Script)

---

## 🧭 실습 전체 흐름 요약

```
[1단계] 학습 스크립트 작성
[2단계] BashOperator로 스크립트 실행 DAG 구성
[3단계] Airflow 웹 UI에서 실행 및 로그 확인
```

---

## 📁 디렉토리 구조

```
airflow/
├── dags/
│   └── run_train_script.py   ← DAG 파일
├── ml_code/
│   ├── train.py              ← 모델 학습 스크립트
│   └── model.pkl             ← 학습된 모델 파일
```

---

## 🧪 [1단계] 학습 스크립트 작성 (`train.py`)

```python
# airflow/ml_code/train.py

import pickle
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

# 데이터 로딩
data = load_iris()
X, y = data.data, data.target

# 모델 학습
model = RandomForestClassifier()
model.fit(X, y)

# 모델 저장
model_path = "/opt/airflow/ml_code/model.pkl"
with open(model_path, "wb") as f:
    pickle.dump(model, f)
```

> 모델은 /opt/airflow/ml_code/model.pkl 경로에 저장됩니다.
> 

---

## 🧪 [2단계] DAG 작성 (`run_train_script.py`)

```python
# airflow/dags/run_train_script.py

from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='bash_run_train',
    start_date=datetime(2023, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:

    run_training = BashOperator(
        task_id='run_train_script',
        bash_command='python3 /opt/airflow/ml_code/train.py'
    )
```

> train.py는 Airflow 컨테이너 내 /opt/airflow/ml_code/ 경로에 위치해야 합니다.
> 

---

## ✅ [3단계] Airflow 컨테이너 진입 및 확인

```bash
docker exec -it airflow-airflow-webserver-1 bash
cd /opt/airflow/ml_code
ls -l
```

> train.py와 model.pkl 파일이 보이면 성공!
> 

---

## 🔧 추가 설정: `requirements.txt`와 Docker 재빌드

1. **requirements.txt** 작성 (Airflow 루트 디렉토리):

```
scikit-learn
```

1. **Dockerfile** 작성 (같은 디렉토리):

```
FROM apache/airflow:2.8.2
COPY requirements.txt /requirements.txt
USER airflow
RUN pip install --no-cache-dir -r /requirements.txt
```

1. **Docker Compose** 재빌드

```bash
docker-compose down
docker-compose build
docker-compose up -d
```

---

## ✅ 실행 절차

1. `train.py` 스크립트 저장
2. `run_train_script.py` DAG 등록
3. Airflow 웹 UI에서 DAG 실행
4. 로그에서 `"📥 데이터 로딩 중..."`, `"🧠 모델 학습 중..."`, `"💾 모델 저장 완료!"` 확인

---

## 🔍 확인 포인트

| 항목 | 확인 방법 |
| --- | --- |
| 모델 파일 생성 | `airflow/ml_code/model.pkl` 존재 확인 |
| 로그 출력 | DAG > Task Logs에서 출력 메시지 확인 |

---

## 🧩 실무 팁

| 항목 | 설명 |
| --- | --- |
| BashOperator | 외부 스크립트 호출 시 사용 (`.sh`, `.py` 등) |
| PythonOperator | DAG 내부 함수 호출 및 실행 |
| 실무 연계 | `python train.py` → `python predict.py` → `upload_to_s3.sh` 등으로 워크플로우 구성 가능 |

---

## 🔧 MLOps 실전 연결

- 이 흐름은 **MLflow Tracking → 모델 등록 → S3 업로드 → SageMaker 배포** 흐름으로 이어질 수 있음
- **Airflow DAG**를 통해 **ML 실험 자동화**, **모델 학습 및 배포 파이프라인 구축**이 가능함
