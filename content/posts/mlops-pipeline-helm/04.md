+++
date = '2025-07-03T17:11:53+09:00'
draft = false
title = '[MLOps 플랫폼 구축 - 4단계: Airflow : GitSync + 외부 PostgreSQL + Secret 연동]'
categories = ['Airflow' ,'PostgreSQL', 'AWS', 'Kubernetes', 'Helm', 'Git', 'NFS']
+++

## 🧠 시나리오 설명

> “실무에서는 데이터 파이프라인이나 모델 학습 작업을 수시로 업데이트하게 되며,
> 
> 이를 수동으로 업로드하지 않고 Git 기반으로 관리하는 것이 필수입니다.
> 
> Airflow는 GitSync 기능을 통해 DAG를 자동 동기화할 수 있습니다.”
> 
- DAG 코드가 Git으로 관리되어야 리뷰, 히스토리, 협업 가능
- GitSync → DAG 자동 배포 (CI/CD 개념 적용)
- Secret으로 Git 인증 → 조직 내 GitOps 문화와 연계

---

## ✨ TL;DR

- Helm을 통해 Airflow를 배포하면서 DAG 코드를 Git 저장소에서 자동으로 동기화하는 구조 설계
- GitSync, Secret 기반 SSH 인증, 외부 PostgreSQL, AWS S3 연동까지 포함해 구성
- UI 접근은 Ingress를 통해 이루어지며, 로그는 PVC 또는 S3로 설정 가능

---

## 📐 아키텍처 구성도

![04](/mlops-journey/images/04.png)

---

## 🐳 커스텀 Airflow 이미지 구성

GitSync DAG에서 MLflow 연동 또는 AWS SDK 사용을 위한 Python 패키지 설치 필요

### 🔸 Dockerfile

```
FROM apache/airflow:3.0.2-python3.12

COPY requirements.txt /tmp/requirements.txt
USER airflow
RUN pip install --no-cache-dir -r /tmp/requirements.txt
```

### 🔸 requirements.txt

```
mlflow==2.13.0
pandas
scikit-learn
boto3
```

### 🔸 빌드 & 푸시

```bash
docker build -t hoizz/airflow-custom:v5-mlflow .
docker push hoizz/airflow-custom:v5-mlflow
```

---

## 🛠 Helm 구성

### 🔹 values.yaml

```yaml
# 실행 엔진 설정
executor: KubernetesExecutor

# 이미지 설정
images:
  airflow:
    repository: hoizz/airflow-custom  # 커스텀 이미지
    tag: v5-mlflow-20240712           # 태그 
    pullPolicy: IfNotPresent

# 생성한 fernetKey 고정 (helm delete & install 이후에도 이전 variables 등 데이터 sync 가능)
fernetKeySecretName: my-fernet-secret

# DAG GitSync 설정 (GitOps 기반)
dags:
  gitSync:
    enabled: true
    repo: git@github.com:keonhoban/airflow-dags.git
    branch: main
    subPath: dags
    depth: 1
    wait: 10
    rev: HEAD
    sshKeySecret: airflow-git-ssh-secret

# DB 연동 (외부 PostgreSQL 사용)
postgresql:
  enabled: false  # 내부 Postgres 비활성화

data:
  metadataSecretName: airflow-db-secret  # Secret 안의 base64된 connection 값 참조

# Airflow 환경 설정
config:
  AIRFLOW__CORE__LOAD_EXAMPLES:
    value: "False"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION:
    value: "False"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG:
    value: "True"

# 리소스 설정 (권장)
scheduler:
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1"
      memory: "1Gi"
  extraVolumes:
    - name: aws-credentials
      secret:
        secretName: aws-credentials-secret
  extraVolumeMounts:
    - name: aws-credentials
      mountPath: /home/airflow/.aws
      readOnly: true

workers:
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1"
      memory: "1Gi"
  extraVolumes:
    - name: aws-credentials
      secret:
        secretName: aws-credentials-secret
  extraVolumeMounts:
    - name: aws-credentials
      mountPath: /home/airflow/.aws
      readOnly: true

dagProcessor:
  extraVolumes:
    - name: aws-credentials
      secret:
        secretName: aws-credentials-secret
  extraVolumeMounts:
    - name: aws-credentials
      mountPath: /home/airflow/.aws
      readOnly: true

# Ingress 설정 (Ingress Controller 기반 접근)
ingress:
  enabled: true

  apiServer:
    enabled: true
    ingressClassName: nginx
    hosts:
      - name: airflow.local
    tls:
      enabled: false  # 실무에서는 true + cert-manager 연동 필요

# Web UI 서비스 타입 설정
apiServer:
  service:
    type: ClusterIP  # 외부 접근은 Ingress 경유

# 로그 PVC 설정
logs:
  persistence:
    enabled: true
    existingClaim: airflow-logs-pvc

# (선택) remote 로그 저장소 설정 (예: S3)
# config:
#   AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
#   AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://your-bucket-name/airflow-logs"
#   AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "aws_default"

```

---

## ✅ `fernet_key` 생성 및 구성

- 

```bash
# 생성
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# secret 적용 (파일 생성 후 kubectl apply 가능)
# (git에 필요한 경우 SealedSecret or SOPS or .gitignore)
apiVersion: v1
kind: Secret
metadata:
  name: my-fernet-secret
  namespace: airflow
type: Opaque
stringData:
  fernet-key: "xZ69xLVkiCfJQ23fXa8oubANRIJRHZLrIeANlHhPGmQ="  # 생성한 값으로 수정
```

---

## 🔐 GitSync SSH Key & Secret 구성

```bash
# SSH 키 생성
ssh-keygen -t rsa -b 4096 -C "testuser@mlops.com"

# GitHub UI에서 Deploy Key 등록 (공개키)

# K8s Secret 생성 (비공개 키)
kubectl create secret generic airflow-git-ssh-secret --from-file=gitSshKey=/root/.ssh/id_rsa -n airflow
```

---

## 🚀 배포 명령어

```bash
kubectl create namespace airflow

helm install airflow apache-airflow/airflow -n airflow -f values.yaml
```

---

## 🌐 접속 확인

```bash
# /etc/hosts 수정
{NodeIP} airflow.local

# 브라우저 접속
http://airflow.local
```

---

## 🧪 확인 사항

| 항목 | 확인 방법 |
| --- | --- |
| Airflow UI 접근 | `http://airflow.local` |
| DAG GitSync | GitHub push → 자동 반영 여부 확인 |
| PostgreSQL 연결 | metadata DB에 연결 로그 확인 |
| AWS 인증 연동 | `boto3.client()` 호출 시 에러 없는지 확인 |

---

## 🧩 Tip

- GitSync는 `depth: 1`, `wait`, `rev: HEAD` 옵션 설정으로 최적화 필요
- `metadataSecretName`에는 `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN`이 base64 인코딩되어 있어야 함
- 각 Pod (`scheduler`, `worker`, `dagProcessor`)마다 AWS credential 마운트 필요
- Ingress 접근을 위해선 hosts 파일에 수동 등록 필요 (로컬 클러스터인 경우)

---

## 🔧 MLOps 실전 연결

| 연결 항목 | 실무 의미 |
| --- | --- |
| DAG GitSync | GitOps 기반으로 실험/운영 동기화 자동화 |
| External PostgreSQL | 운영 DB와 통합, 백업/모니터링 가능 |
| boto3 연동 | DAG 내 S3 접근 가능 (모델 or raw 데이터 연동) |
| mlflow API 사용 | 실험 자동화 가능 (training, promotion 포함) |

---

## 🚨 트러블슈팅

### 🔻 GitSync DAG 미반영시 체크 리스트

- `subPath` 경로 오류
- Secret에 등록된 SSH key 권한 문제
- GitHub deploy key에 write 권한 누락

---

## 🧭 다음 포스트 예고

> ⚙️ FastAPI 서빙 + MLflow 모델 연동
> 
> 
> → 모델 핫스왑, REST API 구성, Stage/Version 자동 적용까지 진행 예정입니다.
>
